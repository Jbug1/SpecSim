{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages internal and external\n",
    "import tests\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import datasetBuilder\n",
    "import tools\n",
    "import scipy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "import sklearn.base\n",
    "import pickle\n",
    "import copy\n",
    "import tests\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "import os\n",
    "import plotAndOrderResults\n",
    "import datasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path variables where we want to save created datasets, intermediate outputs, etc\n",
    "#If you want to run this on your own computer, obviously you should update the path\n",
    "\n",
    "outputs_path='/Users/jonahpoczobutt/projects/specsim_res/Metlin_to_Nist'\n",
    "os.mkdir(outputs_path)\n",
    "\n",
    "\n",
    "nist14='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist14.pkl'\n",
    "gnps='/Users/jonahpoczobutt/projects/raw_data/db_csvs/gnps.pkl'\n",
    "mona='/Users/jonahpoczobutt/projects/raw_data/db_csvs/mona_lc.pkl'\n",
    "metlin='/Users/jonahpoczobutt/projects/raw_data/db_csvs/metlin_experimental.pkl'\n",
    "\n",
    "#Set the query and target dbs\n",
    "#these can be different or the same\n",
    "query = metlin\n",
    "target = nist14\n",
    "\n",
    "#This variable toggles whether we do a full run of the notebook, or if we read in variables created in a previous run\n",
    "fullRun=True\n",
    "\n",
    "#create directories for results\n",
    "if fullRun:\n",
    "    \n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/fullSearchMatches')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/quantileDfs')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/modelDatasets')\n",
    "    os.mkdir(f'{outputs_path}/fig1')\n",
    "    os.mkdir(f'{outputs_path}/fig2')\n",
    "    os.mkdir(f'{outputs_path}/fig3')\n",
    "    os.mkdir(f'{outputs_path}/fig4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: Creating Target and Matches DFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fullRun:\n",
    "\n",
    "    #This should be replaced with a function to read in all the databases\n",
    "    query_ = pd.read_pickle(query)\n",
    "    all_bases = list(set(query_['inchi_base']))\n",
    "\n",
    "    first_bases = all_bases[:int(len(all_bases)/2)]\n",
    "    second_bases = all_bases[int(len(all_bases)/2):]\n",
    "\n",
    "    first_query_ = query_[np.isin(query_['inchi_base'],first_bases)]\n",
    "    first_query_.reset_index(inplace=True)\n",
    "    first_query_.to_pickle(f'{outputs_path}/intermediateOutputs/first_query.pkl')\n",
    "    del(first_query_)\n",
    "\n",
    "    second_query_ = query_[np.isin(query_['inchi_base'],first_bases)]\n",
    "    second_query_.reset_index(inplace=True)\n",
    "    second_query_.to_pickle(f'{outputs_path}/intermediateOutputs/second_query.pkl')\n",
    "    del(second_query_)\n",
    "    del(query_)\n",
    "\n",
    "    \n",
    "    np.save(f'{outputs_path}/intermediateOutputs/first_bases.npy',first_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/second_bases.npy',second_bases)\n",
    "    del(first_bases)\n",
    "    del(second_bases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: Global Performance of Individual Metrics/Weighting Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100002 rows created\n",
      "200010 rows created\n",
      "300013 rows created\n",
      "400012 rows created\n",
      "500070 rows created\n",
      "600019 rows created\n",
      "700012 rows created\n",
      "800040 rows created\n",
      "900033 rows created\n",
      "1000068 rows created\n"
     ]
    }
   ],
   "source": [
    "#these are the ppm windows that we want to test\n",
    "ppm_windows = [10]\n",
    "\n",
    "#this is the size of the sample we take from the full target\n",
    "size=1e6\n",
    "\n",
    "#this is the maximum number of matches we allow for each query, based on the precursor window\n",
    "max_matches=100\n",
    "\n",
    "#adduct match\n",
    "adduct_match=False\n",
    "\n",
    "#Similarity methods and transformation parameters below. Leave sim methods as None to run all\n",
    "noise_threshes=[0.01,0.05,0.1]\n",
    "centroid_tolerance_vals = [0.05, 3]\n",
    "centroid_tolerance_types=['da','ppm']\n",
    "powers=[0.25,1,3,'ent',None]\n",
    "sim_methods=None\n",
    "\n",
    "if fullRun:\n",
    "    #we will evaluate the performace of the individual metrics on a large sample from the\n",
    "    #full target dataset. You can set the size below\n",
    "\n",
    "    #reload queries and target for individual comparison\n",
    "    query_=pd.read_pickle(query)\n",
    "    target_=pd.read_pickle(target)\n",
    "\n",
    "    #shuffle query to ensure random subset\n",
    "    query_.sample(frac=1)\n",
    "\n",
    "    for i in ppm_windows:\n",
    "\n",
    "        matches = datasetBuilder.create_matches_df_new(query_,target_,i,max_matches,size, adduct_match)\n",
    "        matches.to_pickle(f'{outputs_path}/intermediateOutputs/fullSearchMatches/matches_{i}_ppm.pkl')\n",
    "\n",
    "        #comparison on large sample\n",
    "        tests.create_variable_comparisons(\n",
    "                                noise_threshes=noise_threshes,\n",
    "                                centroid_threshes=centroid_tolerance_vals,\n",
    "                                centroid_types=centroid_tolerance_types,\n",
    "                                powers=powers,\n",
    "                                sim_methods=sim_methods,\n",
    "                                matches=matches,\n",
    "                                outpath = f'{outputs_path}/fig1/{i}_ppm.csv',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: Assessing Metric Stability in Smaller Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the size of the sample we take from the full target\n",
    "size=2e3\n",
    "\n",
    "#this is the number of times we want to take a sample of the above size\n",
    "trials=100\n",
    "\n",
    "if fullRun:\n",
    "    #we will evaluate the performace of the individual metrics on a large sample from the\n",
    "    #full target dataset. You can set the size below\n",
    "\n",
    "    #reload target\n",
    "    query_=pd.read_pickle(query)\n",
    "    target_=pd.read_pickle(target)\n",
    "\n",
    "    for i in range(trials):\n",
    "\n",
    "        os.mkdir(f'{outputs_path}/fig2/{i}')\n",
    "        \n",
    "        for j in ppm_windows:\n",
    "\n",
    "            #create matches after shuffling query\n",
    "            query_ = query_.sample(frac=1)\n",
    "            matches = datasetBuilder.create_matches_df_new(query_,target_,j,max_matches,size, adduct_match)\n",
    "\n",
    "            #comparison on small sample\n",
    "            tests.create_variable_comparisons(\n",
    "                                    noise_threshes=noise_threshes,\n",
    "                                    centroid_threshes=centroid_tolerance_vals,\n",
    "                                    centroid_types=centroid_tolerance_types,\n",
    "                                    powers=powers,\n",
    "                                    sim_methods=sim_methods,\n",
    "                                    matches=matches,\n",
    "                                    outpath = f'{outputs_path}/fig2/{i}/{j}_ppm.csv'\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figures 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Metrics for 10_ppm.csv by AUC\n",
      "      0                   1         2                  3\n",
      "361  61          lorentzian  0.680889  0.01_0.05_da_None\n",
      "338  38           hellinger  0.677313  0.01_0.05_da_None\n",
      "61   61          lorentzian  0.675163  0.01_0.05_da_0.25\n",
      "318  18         max_entropy  0.672463  0.01_0.05_da_None\n",
      "327  27  max_bhattacharya_2  0.672041  0.01_0.05_da_None\n",
      "38   38           hellinger  0.666740  0.01_0.05_da_0.25\n",
      "113  38           hellinger  0.665461     0.01_0.05_da_1\n",
      "360  60        max_matusita  0.664686  0.01_0.05_da_None\n",
      "301   1   max_squared_chord  0.664686  0.01_0.05_da_None\n",
      "349  49    lorentzian_jonah  0.664641  0.01_0.05_da_None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first, generate tables from full size\n",
    "plotAndOrderResults.fig1(f'{outputs_path}/fig1','/Users/jonahpoczobutt/projects/specsim_res/Metlin_to_Nist/intermediateOutputs/fullSearchMatches')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ranks and Means for 10 PPM\n",
      "Proportion of Time This Metric is Top\n",
      "                                  index     0\n",
      "20                          max_entropy  0.01\n",
      "18                max_squared_euclidean  0.01\n",
      "17                              entropy  0.01\n",
      "16                     lorentzian_jonah  0.01\n",
      "15                       absolute_value  0.01\n",
      "14                        jensenshannon  0.01\n",
      "6   probabilistic_symmetric_chi_squared  0.01\n",
      "19                        max_manhattan  0.01\n",
      "13                        entropy_jonah  0.02\n",
      "11       whittaker_index_of_association  0.02\n",
      "\n",
      "\n",
      "Mean Ranking By Metric\n",
      "                                  index      0\n",
      "1        whittaker_index_of_association  40.18\n",
      "2                           max_entropy  19.92\n",
      "12                        max_manhattan  49.01\n",
      "17                        entropy_jonah  22.76\n",
      "18                              entropy  22.67\n",
      "19                        jensenshannon  23.16\n",
      "20  probabilistic_symmetric_chi_squared  22.03\n",
      "26                     lorentzian_jonah  19.17\n",
      "31                       absolute_value  25.43\n",
      "60                max_squared_euclidean  44.05\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#assess metric stability from small runs\n",
    "plotAndOrderResults.fig2(f'{outputs_path}/fig2', ppm_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3: Hierarchy of Metric Performance Conditional Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will only consider the default weighting scheme here\n",
    "#consider all the below metrics\n",
    "metrics=['lorentzian','lorentzian_jonah','entropy','max_entropy','hellinger','max_bhattacharya_2','cosine','laplacian']\n",
    "quantile_variables=['entropy','num_peaks','precquery','']\n",
    "quantile_num=10\n",
    "output_path=f'{outputs_path}/fig3'\n",
    "\n",
    "for i in ppm_windows:\n",
    "\n",
    "    matches = pd.read_pickle(f'{outputs_path}/intermediateOutputs/fullSearchMatches/{i}_ppm.pkl')\n",
    "    input_data = datasetBuilder.create_model_dataset(matches,\n",
    "                                                     sim_methods=metrics,\n",
    "                                                     noise_threshes=[0.01],\n",
    "                                                     centroid_tolerance_vals=[0.05],\n",
    "                                                     centroid_tolerance_types=[\"da\"],\n",
    "                                                     powers=[1],)\n",
    "    \n",
    "    plotAndOrderResults.fig3(input_data, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4: Combining Features Into Learned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: Create Train and Test datasets for each ppm window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100024 rows created\n",
      "200004 rows created\n",
      "300018 rows created\n",
      "400012 rows created\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m matches \u001b[39m=\u001b[39m datasetBuilder\u001b[39m.\u001b[39mcreate_matches_df_new(query_test,target_,i,max_matches,test_size, adduct_match)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdel\u001b[39;00m(query_test)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m test \u001b[39m=\u001b[39m datasetBuilder\u001b[39m.\u001b[39mcreate_model_dataset(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m                                             matches, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m                                             sim_methods, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                                             noise_threshes, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                                             centroid_tolerance_vals, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                                             centroid_tolerance_types,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                                             powers\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                                             )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m test\u001b[39m.\u001b[39mto_pickle(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00moutputs_path\u001b[39m}\u001b[39;00m\u001b[39m/fig2/supplementary/test_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_ppm.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonahpoczobutt/projects/SpecSim/figures.ipynb#X16sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m matches\u001b[39m.\u001b[39mto_pickle(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00moutputs_path\u001b[39m}\u001b[39;00m\u001b[39m/fig2/supplementary/test_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_ppm_matches.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/SpecSim/datasetBuilder.py:1795\u001b[0m, in \u001b[0;36mcreate_model_dataset\u001b[0;34m(matches_df, sim_methods, noise_threshes, centroid_tolerance_vals, centroid_tolerance_types, powers)\u001b[0m\n\u001b[1;32m   1789\u001b[0m sim_columns_ \u001b[39m=\u001b[39m [\n\u001b[1;32m   1790\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mcentroid_tolerance_vals[k]\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mcentroid_tolerance_types[k]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m sim_methods\n\u001b[1;32m   1792\u001b[0m ]\n\u001b[1;32m   1794\u001b[0m \u001b[39m# clean specs and get corresponding spec features\u001b[39;00m\n\u001b[0;32m-> 1795\u001b[0m cleaned_df \u001b[39m=\u001b[39m matches_df\u001b[39m.\u001b[39mapply(\n\u001b[1;32m   1796\u001b[0m     \u001b[39mlambda\u001b[39;00m x: clean_and_spec_features(\n\u001b[1;32m   1797\u001b[0m         x[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1798\u001b[0m         x[\u001b[39m\"\u001b[39m\u001b[39mprecquery\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1799\u001b[0m         x[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1800\u001b[0m         x[\u001b[39m\"\u001b[39m\u001b[39mprectarget\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1801\u001b[0m         noise_thresh\u001b[39m=\u001b[39mi,\n\u001b[1;32m   1802\u001b[0m         centroid_thresh\u001b[39m=\u001b[39mcentroid_tolerance_vals[k],\n\u001b[1;32m   1803\u001b[0m         power\u001b[39m=\u001b[39mj,\n\u001b[1;32m   1804\u001b[0m     ),\n\u001b[1;32m   1805\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1806\u001b[0m     result_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpand\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1807\u001b[0m )\n\u001b[1;32m   1809\u001b[0m cleaned_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m (\n\u001b[1;32m   1810\u001b[0m     spec_columns_  \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1811\u001b[0m )\n\u001b[1;32m   1813\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(cleaned_df)):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_series_generator()\n\u001b[1;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf(v)\n\u001b[1;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/SpecSim/datasetBuilder.py:1796\u001b[0m, in \u001b[0;36mcreate_model_dataset.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1789\u001b[0m sim_columns_ \u001b[39m=\u001b[39m [\n\u001b[1;32m   1790\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mcentroid_tolerance_vals[k]\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mcentroid_tolerance_types[k]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m sim_methods\n\u001b[1;32m   1792\u001b[0m ]\n\u001b[1;32m   1794\u001b[0m \u001b[39m# clean specs and get corresponding spec features\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m cleaned_df \u001b[39m=\u001b[39m matches_df\u001b[39m.\u001b[39mapply(\n\u001b[0;32m-> 1796\u001b[0m     \u001b[39mlambda\u001b[39;00m x: clean_and_spec_features(\n\u001b[1;32m   1797\u001b[0m         x[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1798\u001b[0m         x[\u001b[39m\"\u001b[39m\u001b[39mprecquery\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1799\u001b[0m         x[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1800\u001b[0m         x[\u001b[39m\"\u001b[39m\u001b[39mprectarget\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1801\u001b[0m         noise_thresh\u001b[39m=\u001b[39mi,\n\u001b[1;32m   1802\u001b[0m         centroid_thresh\u001b[39m=\u001b[39mcentroid_tolerance_vals[k],\n\u001b[1;32m   1803\u001b[0m         power\u001b[39m=\u001b[39mj,\n\u001b[1;32m   1804\u001b[0m     ),\n\u001b[1;32m   1805\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1806\u001b[0m     result_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpand\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1807\u001b[0m )\n\u001b[1;32m   1809\u001b[0m cleaned_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m (\n\u001b[1;32m   1810\u001b[0m     spec_columns_  \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1811\u001b[0m )\n\u001b[1;32m   1813\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(cleaned_df)):\n",
      "File \u001b[0;32m~/projects/SpecSim/datasetBuilder.py:1216\u001b[0m, in \u001b[0;36mclean_and_spec_features\u001b[0;34m(spec1, prec1, spec2, prec2, noise_thresh, centroid_thresh, centroid_type, power)\u001b[0m\n\u001b[1;32m   1213\u001b[0m spec2_ \u001b[39m=\u001b[39m tools\u001b[39m.\u001b[39mweight_intensity(spec2_, power)\n\u001b[1;32m   1215\u001b[0m \u001b[39m# get new spec features\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m spec_features \u001b[39m=\u001b[39m get_spec_features(spec1_, prec1, spec2_, prec2)\n\u001b[1;32m   1218\u001b[0m \u001b[39m# get spec change features\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[39m# spec1_features = get_spec_change_features(spec1, spec1_)\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[39m# spec2_features = get_spec_change_features(spec2, spec2_)\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m spec1_ \u001b[39m=\u001b[39m tools\u001b[39m.\u001b[39mstandardize_spectrum(spec1_)\n",
      "File \u001b[0;32m~/projects/SpecSim/datasetBuilder.py:971\u001b[0m, in \u001b[0;36mget_spec_features\u001b[0;34m(spec_query, precursor_query, spec_target, precursor_target)\u001b[0m\n\u001b[1;32m    968\u001b[0m spec_query \u001b[39m=\u001b[39m spec_query[below_prec_indices]\n\u001b[1;32m    970\u001b[0m n_peaks \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(spec_query)\n\u001b[0;32m--> 971\u001b[0m ent \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mentropy(spec_query[:, \u001b[39m1\u001b[39m])\n\u001b[1;32m    973\u001b[0m outrow[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m ent\n\u001b[1;32m    974\u001b[0m outrow[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m n_peaks\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_entropy.py:143\u001b[0m, in \u001b[0;36mentropy\u001b[0;34m(pk, qk, base, axis)\u001b[0m\n\u001b[1;32m    141\u001b[0m     qk \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m*\u001b[39mqk \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(qk, axis\u001b[39m=\u001b[39maxis, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m     vec \u001b[39m=\u001b[39m special\u001b[39m.\u001b[39mrel_entr(pk, qk)\n\u001b[0;32m--> 143\u001b[0m S \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(vec, axis\u001b[39m=\u001b[39maxis)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m base \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     S \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(base)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2188\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum_dispatcher\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2184\u001b[0m                     initial\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2185\u001b[0m     \u001b[39mreturn\u001b[39;00m (a, out)\n\u001b[0;32m-> 2188\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2190\u001b[0m         initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2191\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2192\u001b[0m \u001b[39m    Sum of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2193\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[39m    15\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2311\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, _gentype):\n\u001b[1;32m   2312\u001b[0m         \u001b[39m# 2018-02-25, 1.15.0\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Similarity methods and transformation parameters below. Leave sim methods as None to run all\n",
    "noise_threshes=[0.01,0.05,0.1]\n",
    "centroid_tolerance_vals = [0.05,0.1]\n",
    "centroid_tolerance_types=['da', 'da']\n",
    "powers=[0.25,1,3,'ent',None]\n",
    "sim_methods=['lorentzian','lorentzian_jonah','entropy','max_entropy','hellinger','max_bhattacharya_2']\n",
    "\n",
    "train_size=1.1e6\n",
    "test_size=5e5\n",
    "\n",
    "max_matches=50\n",
    "\n",
    "target_=pd.read_pickle(target)\n",
    "\n",
    "for i in ppm_windows:\n",
    "\n",
    "    #read in first bases and shuffle order\n",
    "    query_train = pd.read_pickle(f'{outputs_path}/intermediateOutputs/first_query.pkl')\n",
    "    query_train=query_train.sample(frac=1)\n",
    "\n",
    "    #create matches for model to train on\n",
    "    matches = datasetBuilder.create_matches_df_new(query_train,target_,i,max_matches,train_size, adduct_match)\n",
    "    del(query_train)\n",
    "\n",
    "    train = datasetBuilder.create_model_dataset(\n",
    "                                                matches, \n",
    "                                                sim_methods, \n",
    "                                                noise_threshes, \n",
    "                                                centroid_tolerance_vals, \n",
    "                                                centroid_tolerance_types,\n",
    "                                                powers\n",
    "                                                )\n",
    "    \n",
    "    train.to_pickle(f'{outputs_path}/intermediateOutpus/model_data/train_{i}_ppm.pkl')\n",
    "    del(matches)\n",
    "    del(train)\n",
    "\n",
    "    #read in test queries and shuffle order\n",
    "    query_test = pd.read_pickle(f'{outputs_path}/intermediateOutputs/second_query.pkl')\n",
    "    query_test=query_test.sample(frac=1)\n",
    "\n",
    "    #create matches for model to train on\n",
    "    matches = datasetBuilder.create_matches_df_new(query_test,target_,i,max_matches,test_size, adduct_match)\n",
    "\n",
    "    del(query_test)\n",
    "\n",
    "    test = datasetBuilder.create_model_dataset(\n",
    "                                                matches, \n",
    "                                                sim_methods, \n",
    "                                                noise_threshes, \n",
    "                                                centroid_tolerance_vals, \n",
    "                                                centroid_tolerance_types,\n",
    "                                                powers\n",
    "                                                )\n",
    "    \n",
    "    test.to_pickle(f'{outputs_path}/intermediateOutpus/modelDatasets/test_{i}_ppm.pkl')\n",
    "    matches.to_pickle(f'{outputs_path}/intermediateOutpus/modelDatasets/test_{i}_ppm_matches.pkl')\n",
    "    del(matches)\n",
    "    del(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create the indices we want to test (different features for model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=dict()\n",
    "\n",
    "noise_threshes=[True,True,True]\n",
    "centroid_tolerance_values=[True,True]\n",
    "powers=[True,True,True,True,True]\n",
    "spec_features=[True,True, True, True, True, True, True, True]#8\n",
    "sim_methods=[True,True,True,True]#7\n",
    "\n",
    "indices['all'] = datasetBuilder.generate_keep_indices(noise_threshes,\n",
    "                                                      centroid_tolerance_vals,\n",
    "                                                      powers,\n",
    "                                                      spec_features,\n",
    "                                                      sim_methods,\n",
    "                                                      True,\n",
    "                                                      True,\n",
    "                                                      True\n",
    "                                                    )\n",
    "\n",
    "indices['all_but_nonspec'] = datasetBuilder.generate_keep_indices(noise_threshes,\n",
    "                                                      centroid_tolerance_vals,\n",
    "                                                      powers,\n",
    "                                                      spec_features,\n",
    "                                                      sim_methods,\n",
    "                                                      True,\n",
    "                                                      False,\n",
    "                                                      True\n",
    "                                                    )\n",
    "\n",
    "indices['nonspec_only']=list(range(9))\n",
    "indices['nonspec_no_precursor']=[2,3,4,5,6]\n",
    "\n",
    "\n",
    "sim_methods=[False, False, False, False]#7\n",
    "indices['no_sims'] = datasetBuilder.generate_keep_indices(noise_threshes,\n",
    "                                                      centroid_tolerance_vals,\n",
    "                                                      powers,\n",
    "                                                      spec_features,\n",
    "                                                      sim_methods,\n",
    "                                                      False,\n",
    "                                                      True,\n",
    "                                                      True\n",
    "                                                    )\n",
    "\n",
    "noise_threshes=[True,True,True]\n",
    "centroid_tolerance_values=[True,True]\n",
    "powers=[True,True,True,True,True]\n",
    "spec_features=[False for i in range(8)]#8\n",
    "sim_methods=[True,True,True,True]#7\n",
    "\n",
    "indices['sims_only'] = datasetBuilder.generate_keep_indices(noise_threshes,\n",
    "                                                      centroid_tolerance_vals,\n",
    "                                                      powers,\n",
    "                                                      spec_features,\n",
    "                                                      sim_methods,\n",
    "                                                      False,\n",
    "                                                      True,\n",
    "                                                      False,\n",
    "                                                    )\n",
    "\n",
    "\n",
    "noise_threshes=[True,False,True]\n",
    "centroid_tolerance_values=[True,False]\n",
    "powers=[True,False,True,False,True]\n",
    "spec_features=[True for i in range(8)]#8\n",
    "sim_methods=[True,True,False, False]#7\n",
    "\n",
    "indices['small_model'] = datasetBuilder.generate_keep_indices(noise_threshes,\n",
    "                                                      centroid_tolerance_vals,\n",
    "                                                      powers,\n",
    "                                                      spec_features,\n",
    "                                                      sim_methods,\n",
    "                                                      False,\n",
    "                                                      True,\n",
    "                                                      True,\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import train, val and test\n",
    "train = pd.read_pickle(f'{outputs_path}/intermediateOutpus/modelDatasets/train_{i}_ppm.pkl')\n",
    "test = pd.read_pickle(f'{outputs_path}/intermediateOutpus/modelDatasets/test_{i}_ppm.pkl')\n",
    "val=train[:int(1e5)]\n",
    "train=train[int(1e5):]\n",
    "\n",
    "#hyperparam specifications for models\n",
    "models = [\n",
    "        hgbc(),\n",
    "        hgbc(learning_rate=0.01),\n",
    "        hgbc(learning_rate=0.5),\n",
    "        hgbc(max_iter=200),\n",
    "        hgbc(learning_rate=0.01, max_iter=200),\n",
    "        hgbc(learning_rate=0.5, max_iter=200),\n",
    "        hgbc(min_samples_leaf=10),\n",
    "        hgbc(learning_rate=0.01,min_samples_leaf=10),\n",
    "        hgbc(learning_rate=0.5,min_samples_leaf=10),\n",
    "        hgbc(max_iter=200,min_samples_leaf=10),\n",
    "        hgbc(learning_rate=0.01, max_iter=200,min_samples_leaf=10),\n",
    "        hgbc(learning_rate=0.5, max_iter=200,min_samples_leaf=10),\n",
    "        ]\n",
    "\n",
    "results_by_subset = tests.best_models_by_subset(indices, [int(1e6)], models, train, val, test)\n",
    "\n",
    "del(train)\n",
    "del(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
